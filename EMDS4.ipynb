{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoxIFqE4yzXtQXHaxuc3qI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S14vcGt/learning-notebooks/blob/main/EMDS4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4. Linear Algebra\n",
        "\n",
        "Linear algebra is a topic I've seen in college classes, it's a very wide field and something I find quite atractive about it is how many applications it has in science and engeenering, vectors and matrices are everywhere!"
      ],
      "metadata": {
        "id": "5ZbdHxWeYMPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectors\n",
        "\n",
        " A vector is a one-dimension data structure, usually abstrayed as a list, at least from a computer science perspective. In general, a vector is a way of putting together data. You know they are talking about vectors when you see a little arrow above a normal letter : $\\vec a$. Every code example is going to be done using NumPy, since native python is slow and not as powerfull as dedicated libreries like NumPy, Pandas and others."
      ],
      "metadata": {
        "id": "4ClwC0-uZKO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx1RsfVcYIdK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "boring_slow_vector = [3,4]\n",
        "cool_fast_vector = np.array([3,4])\n",
        "print(boring_slow_vector)\n",
        "cool_fast_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are several vector operations that have a mathematical and visual explanation, quite interesting to study and see by the way, but I want to use this chapter to practice with numpy, s"
      ],
      "metadata": {
        "id": "hCbHFtFhcumR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vector addition or substraccion\n",
        "$$\\vec a \\pm \\vec b = (a_1\\pm b_1,a_2 \\pm b_2,… a_n±b_n)$$\n",
        "\n",
        "You must note$\\quad\\vec a - \\vec b \\neq \\vec b - \\vec a$"
      ],
      "metadata": {
        "id": "5urY4sL6eBg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "\n",
        "v = array([3,2])\n",
        "w = array([2,-1])\n",
        "\n",
        "print(v + w)\n",
        "print(v-w)\n",
        "w-v"
      ],
      "metadata": {
        "id": "XGeuLmW0fhk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling\n",
        "In linear algebra, common numberas are called scalars, so when you want to multiplya a normal number with a vector, its called scalar multiplication.\n",
        "\n",
        "$$ \\alpha\\vec v= \\begin{bmatrix} \\alpha \\times v_1\\\\ \\alpha \\times v_2\\\\ \\vdots \\\\ \\alpha \\times v_n\\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "ViSm0_Pzf9B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "\n",
        "v = array([3,2])\n",
        "\n",
        "2 * v"
      ],
      "metadata": {
        "id": "NaWcH0D8i9BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Span and Linear Dependence\n",
        " These two operations, adding two vectors and scaling them, brings about a\n",
        " simple but powerful idea. With these two operations, we can combine two\n",
        " vectors and scale them to create any resulting vector we want.\n",
        "\n",
        "  This whole space of possible vectors is called span, and in most cases our\n",
        " span can create unlimited vectors off even only a couple of vectors, simply by scaling\n",
        " and summing them. When we have two vectors in two different directions,\n",
        " they are linearly independent and have this unlimited span.\n",
        "\n",
        " What happens when two vectors exist in the same direction, or exist on the\n",
        " same line? The combination of those vectors is also stuck on the same line,\n",
        " limiting our span to just that line. No matter how you scale it, the resulting\n",
        " sum vector is also stuck on that same line. This makes them linearly\n",
        " dependent.\n",
        "  The span here is stuck on the same line as the two vectors it is made out of.\n",
        " Because the two vectors exist on the same underlying line, we cannot\n",
        " flexibly create any new vector through scaling. Any resulting vector is\n",
        " stuck on that line.\n",
        " In 3 or more dimensions, when we have a linearly dependent set of vectors\n",
        " we often get stuck on a plane in a smaller number of dimensions. A lot of problems become difficult or unsolvable when they are linearly dependent.\n",
        "\n",
        " Getting away from context a little bit, this principle of linear independency has taken more protagonism recently, as it is the fundamental concept behind a new cryptographic algorithm that is intended to be unbreackable by quantum computers, you can read more about it [here](https://pq-crystals.org/kyber/index.shtml)"
      ],
      "metadata": {
        "id": "jVa8MyC7jO0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Transformations\n",
        "\n",
        "They are a set of operations that let us create new vectors in different ways, like scaling, rotation, shearing, and inversion.  Scaling a vector will stretch or squeeze it. Rotations will turn the vector\n",
        " space, and inversions will flip the vector space so that the basis vectors $( \\hat i \\text{ and } \\hat j)$ swap respective places."
      ],
      "metadata": {
        "id": "9Y2YHynjpi00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix vector multiplication\n",
        "\n",
        "The formula to transform a vector $\\vec v$ given basis vectors $\\hat i $\n",
        " and $\\hat j$ packaged as a matrix is as follows: $$\\begin{bmatrix} x_{new}\\\\ y_{new}\\end{bmatrix} = \\begin{bmatrix} a & b\\\\ c & d \\end{bmatrix} \\begin{bmatrix} x\\\\y \\end{bmatrix}$$  \n",
        " $$\\begin{bmatrix} x_{new}\\\\ y_{new}\\end{bmatrix} = \\begin{bmatrix} ax + by\\\\ cx + dy \\end{bmatrix}$$\n",
        "\n",
        "\n",
        "$\\hat i $ is the first column $[a,c]$ and $\\hat j$ is the second one $[b,d]$. We package both\n",
        " of these basis vectors as a **matrix**, which is a collection of vectors\n",
        " expressed as a grid of numbers in two or more dimensions."
      ],
      "metadata": {
        "id": "sHMSWxajN-yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for this, we also make use of the dot product, which is a common operation between vectors\n",
        "from numpy import array\n",
        "\n",
        "basis = array([[3,0],\n",
        "               [0,2]])\n",
        "v = array([1,1])\n",
        "\n",
        "basis.dot(v)"
      ],
      "metadata": {
        "id": "9vc5P3a4Rhfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as we can see, numpy populate the arrays with rows, not with columns\n",
        "# if we want to declare basis as columns, we can transpose the matrix\n",
        "\n",
        "from numpy import array\n",
        "\n",
        "i_hat = array([2, 0])\n",
        "j_hat = array([0, 3])\n",
        "\n",
        "basis = array([i_hat, j_hat]).transpose()\n",
        "\n",
        "v = array([1,1])\n",
        "\n",
        "basis.dot(v)"
      ],
      "metadata": {
        "id": "knEYuLKcStrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix multiplication\n",
        "\n",
        "You multiply and add\n",
        " each row from the first matrix to each respective column of the second\n",
        " matrix, in an “over-and-down! over-and-down!” pattern, so we can actually consolidate these two separate transformations (rotation\n",
        " and shear) into a single transformation.\n",
        "\n",
        " $$\\begin{bmatrix} a & b\\\\ c & d\\end{bmatrix} \\begin{bmatrix} e & f\\\\ g & h\\end{bmatrix} = \\begin{bmatrix} ae + bg & af + bh\\\\ ce + dg & cf + dh \\end{bmatrix}$$\n",
        "\n",
        " It has to be said the obvius, matrix multiplication is not conmutative."
      ],
      "metadata": {
        "id": "_qRCA1GtU1dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "\n",
        "# Transformation 1\n",
        "i_hat1 = array([0, 1])\n",
        "j_hat1 = array([-1, 0])\n",
        "transform1 = array([i_hat1, j_hat1]).transpose()\n",
        "\n",
        "# Transformation 2\n",
        "i_hat2 = array([1, 0])\n",
        "j_hat2 = array([1, 1])\n",
        "transform2 = array([i_hat2, j_hat2]).transpose()\n",
        "\n",
        "# Combine Transformations\n",
        "combined = transform2 @ transform1\n",
        "\n",
        "# Test\n",
        "print(f\"COMBINED MATRIX:\\n {combined}\")\n",
        "\n",
        "v = array([1, 2])\n",
        "combined.dot(v)"
      ],
      "metadata": {
        "id": "6vZPKaR5Wocx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determinants\n",
        "\n",
        "Determinants describe how much a sampled area in a vector\n",
        " space changes in scale with linear transformations, and this can provide\n",
        " helpful information about the transformation. The most critical piece of information the determinant tells you is whether the transformation is linearly dependent. If you have a\n",
        " determinant of 0 that means all of space has been squished into a lesser\n",
        " dimension. To $\\begin{bmatrix} 3 &0 \\\\ 0 &2 \\end{bmatrix}$ would be:\n"
      ],
      "metadata": {
        "id": "h2saaPgiYYys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import det\n",
        "from numpy import array\n",
        "i_hat = array([3, 0])\n",
        "j_hat = array([0, 2])\n",
        "basis = array([i_hat, j_hat]).transpose()\n",
        "determinant = det(basis)\n",
        "determinant"
      ],
      "metadata": {
        "id": "OVKkMYTy5kY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Systems of Equations\n",
        "\n",
        "One of the basic use cases for linear algebra is solving systems of equations. Let's\n",
        " say you are provided with the following equations and you need to solve for x, y, and z. $$4x+2y+4z=44\\\\5x+3y+7z=56\\\\9x+3y+6z=72$$\n",
        "\n",
        "\n",
        " Extract\n",
        " the coefficients into matrix A, the values on the right-side of the equation\n",
        " into matrix B, and the unknown variables into matrix X.\n",
        "\n",
        " $$A = \\begin{bmatrix} 4 &2 &4 \\\\ 5 & 3 & 7\\\\9 & 3 & 6\\end{bmatrix}\\quad B= \\begin{bmatrix} 44\\\\ 56\\\\72\\end{bmatrix} \\quad  X= \\begin{bmatrix} x\\\\y\\\\z \\end{bmatrix}$$\n",
        "\n",
        "  The function for a linear system of equations is $AX = B$, we need to\n",
        " transform matrix A with some other matrix X that will result in matrix B.\n",
        " We need to “undo” A so we can isolate X and get the values for x, y, and z.\n",
        " The way you undo A is to take $A^{-1}$ and apply it to A via matrix multiplication.\n",
        "\n",
        " $$AX=B\\\\ A^{-1}AX=A^{-1}B\\\\ X=A^{-1}B$$\n"
      ],
      "metadata": {
        "id": "lSEZYLREb3J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from numpy.linalg import inv\n",
        "# 4x + 2y + 4z = 44\n",
        "# 5x + 3y + 7z = 56\n",
        "# 9x + 3y + 6z = 72\n",
        "A = array([\n",
        "[4, 2, 4],\n",
        "[5, 3, 7],\n",
        "[9, 3, 6]\n",
        "])\n",
        "B = array([ 44, 56, 72])\n",
        "X = inv(A).dot(B)\n",
        "\n",
        "X"
      ],
      "metadata": {
        "id": "n40q059vfiyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigenvectors and Eigenvalues\n",
        "\n",
        " Matrix decomposition is breaking up a matrix into its basic components,\n",
        " much like factoring numbers. There are many ways to decompose a matrix, a very common one is called eigendecomposition, it only works on square matrices.\n",
        "\n",
        " In eigendecomposition, there are two components: the eigenvalues denoted by $λ$ and eigenvector by $v$\n",
        "\n",
        " If we have a square matrix A, it has the following eigenvalue equation: $Av =λv$  \n",
        " If A is the original matrix, it is composed of eigenvector $v$ and eigenvalue $λ$.\n",
        " There is one eigenvector and eigenvalue for each dimension of the parent\n",
        " matrix."
      ],
      "metadata": {
        "id": "W7JZqbWuf5s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, diag\n",
        "from numpy.linalg import eig, inv\n",
        "A = array([\n",
        "[1, 2],\n",
        "[4, 5]\n",
        "])\n",
        "eigenvals, eigenvecs = eig(A)\n",
        "print(f\"EIGENVALUES: {eigenvals}\")\n",
        "print(f\"\\nEIGENVECTORS: {eigenvecs}\")"
      ],
      "metadata": {
        "id": "nwzyIYWsszaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how do we rebuild matrix A from the eigenvectors and eigenvalues?\n",
        "We need to make a few tweaks to the formula to reconstruct A:\n",
        "  $A=QΛQ^{-1}$\n",
        "\n",
        " In this new formula, $Q$ is the eigenvectors, $Λ$\n",
        " is the eigenvalues in diagonal form, and $Q^{-1}$ is the inverse matrix of $Q$"
      ],
      "metadata": {
        "id": "C9MfjCApszzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, diag\n",
        "from numpy.linalg import eig, inv\n",
        "A = array([\n",
        "[1, 2],\n",
        "[4, 5]\n",
        "])\n",
        "eigenvals, eigenvecs = eig(A)\n",
        "print(f\"EIGENVALUES: {eigenvals}\\n\")\n",
        "print(f\"EIGENVECTORS: {eigenvecs}\\n\")\n",
        "\n",
        "print(\"ORIGINAL MATRIX: \\n\")\n",
        "eigenvecs @ diag(eigenvals) @ inv(eigenvecs)"
      ],
      "metadata": {
        "id": "Ficp_oAeu3-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}